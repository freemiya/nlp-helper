{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "BERT LM prediction\n",
    "https://github.com/huggingface/transformers/blob/master/docs/source/quickstart.md#bert-example\n",
    "\n",
    "Masking script\n",
    "https://github.com/huggingface/pytorch-pretrained-BERT/blob/f9cde97b313c3218e1b29ea73a42414dfefadb40/examples/lm_finetuning/simple_lm_finetuning.py#L276-L301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import copy\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence_bertstyle(tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of str, tokenized sentence.\n",
    "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n",
    "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n",
    "    \"\"\"\n",
    "    output_label = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = \"[MASK]\"\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # append current token to output (we will predict these later)\n",
    "            try:\n",
    "                output_label.append(tokenizer.vocab[token])\n",
    "            except KeyError:\n",
    "                # For unknown words (should not occur with BPE vocab)\n",
    "                output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "                print(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, output_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_processor:\n",
    "    def __init__(self, modeltype='bert-base-uncased'):\n",
    "        \n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(f'{modeltype}')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertForMaskedLM.from_pretrained(f'{modeltype}')\n",
    "        self.model.eval()\n",
    "        \n",
    "    def prepare_input(self, modelname:str, text: str):\n",
    "        if modelname==\"BERT\":\n",
    "            # Tokenize input\n",
    "            tokenized_text = self.tokenizer.tokenize(text)\n",
    "            tokenized_text = ['[CLS]']  +  tokenized_text +['[SEP]']\n",
    "            tokenized_text_ = copy.copy(tokenized_text)\n",
    "            masktokenized_text, mask_labels = mask_sentence_bertstyle(tokenized_text_, self.tokenizer)\n",
    "\n",
    "            # Convert token to vocabulary indices\n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(masktokenized_text)\n",
    "\n",
    "            # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "            # segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "            segments_ids = [0]* len(indexed_tokens)\n",
    "\n",
    "            # Convert inputs to PyTorch tensors\n",
    "            input_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "                        \n",
    "            return tokenized_text, masktokenized_text, input_tensor,\\\n",
    "                   mask_labels, segments_tensors\n",
    "        \n",
    "    def predict(self, tokenized_text, masktokenized_text, \\\n",
    "                input_tensor, mask_labels, segments_tensors=None):\n",
    "        \n",
    "        input_sentence = self.tokenizer.convert_ids_to_tokens(input_tensor[0].tolist())\n",
    "        sent_length = len(input_sentence)\n",
    "        print(tokenized_text)\n",
    "        print(input_sentence)\n",
    "\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "        if not segments_tensors == None:\n",
    "            segments_tensors = segments_tensors.to('cuda')\n",
    "\n",
    "        self.model.to('cuda')\n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            if not segments_tensors==None:\n",
    "                outputs = self.model(input_tensor,token_type_ids=segments_tensors)\n",
    "            else:\n",
    "                outputs = self.model(input_tensor)\n",
    "\n",
    "            #outputs => Tuple((batchsize, seq_len, vocab_size))\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        # using list comprehension + enumerate() \n",
    "        # index of matching element \n",
    "        mask_positions = [idx for idx, val in enumerate(mask_labels) if val > -1] \n",
    "        print(mask_positions)\n",
    "\n",
    "        actual_words = [val for idx, val in enumerate(tokenized_text) if idx in mask_positions]\n",
    "        fake_words = [input_sentence[mp] for mp in mask_positions]\n",
    "\n",
    "        for mask_position in list(mask_positions):\n",
    "            # confirm we were able to predict the actual words\n",
    "            predicted_index = torch.argmax(predictions[0, mask_position]).item()\n",
    "            predicted_token = self.tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "            input_sentence[mask_position] = predicted_token\n",
    "\n",
    "        print(input_sentence)\n",
    "\n",
    "        print(\"Actual one is \",actual_words)\n",
    "        print(\"Fake one is  \",fake_words)\n",
    "        predicted_words = [input_sentence[mp] for mp in mask_positions]\n",
    "        print(\"Predicted one is  \",predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BERT_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The White House statement said that these changes would help \\\n",
    "        protect the salaries of American workers and ensure that foreign \\\n",
    "        labour coming into the US is high-skilled and do not undercut \\\n",
    "        the United States labour market.'\n",
    "tokenized_text, masktokenized_text, input_tensor,\\\n",
    "                   mask_labels, segments_tensors = processor.prepare_input('BERT', f'{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'white', 'house', 'statement', 'said', 'that', 'these', 'changes', 'would', 'help', 'protect', 'the', 'salaries', 'of', 'american', 'workers', 'and', 'ensure', 'that', 'foreign', 'labour', 'coming', 'into', 'the', 'us', 'is', 'high', '-', 'skilled', 'and', 'do', 'not', 'under', '##cut', 'the', 'united', 'states', 'labour', 'market', '.', '[SEP]']\n",
      "['[CLS]', 'the', 'white', 'house', 'statement', 'said', 'that', 'these', 'changes', 'would', 'help', 'protect', '[MASK]', 'salaries', 'of', '[MASK]', 'workers', 'and', 'ensure', '[MASK]', 'foreign', 'labour', 'coming', 'into', 'the', 'us', 'is', 'high', '-', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'under', '##cut', 'the', 'united', 'states', 'labour', 'market', '.', '[SEP]']\n",
      "[3, 12, 15, 19, 20, 29, 30, 31, 32]\n",
      "['[CLS]', 'the', 'white', 'house', 'statement', 'said', 'that', 'these', 'changes', 'would', 'help', 'protect', 'the', 'salaries', 'of', 'foreign', 'workers', 'and', 'ensure', 'that', 'foreign', 'labour', 'coming', 'into', 'the', 'us', 'is', 'high', '-', 'skilled', 'and', 'and', 'would', 'under', '##cut', 'the', 'united', 'states', 'labour', 'market', '.', '[SEP]']\n",
      "Actual one is  ['house', 'the', 'american', 'that', 'foreign', 'skilled', 'and', 'do', 'not']\n",
      "Fake one is   ['house', '[MASK]', '[MASK]', '[MASK]', 'foreign', '[MASK]', '[MASK]', '[MASK]', '[MASK]']\n",
      "Predicted one is   ['house', 'the', 'foreign', 'that', 'foreign', 'skilled', 'and', 'and', 'would']\n"
     ]
    }
   ],
   "source": [
    "processor.predict(tokenized_text, masktokenized_text, input_tensor,\\\n",
    "                   mask_labels, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
