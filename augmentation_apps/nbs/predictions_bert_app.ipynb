{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "model_dict = {'bert':(bert_tokenizer, bert_model)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return tokens[:top_clean]\n",
    "\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text_sentence, mask_positions, top_clean=5):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "        Get the masked sentence.\n",
    "        Encode it & pass it through model\n",
    "        Now, decode at each position.\n",
    "    \"\"\"\n",
    "    # ========================= BERT =================================\n",
    "    input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
    "    new_sentences = []\n",
    "    for i in range(top_clean): \n",
    "        predicted_sentence = ('[CLS] '+text_sentence+' [SEP]').strip().split()\n",
    "        new_sentences.append(predicted_sentence)\n",
    "    \n",
    "    print(new_sentences)\n",
    "    with torch.no_grad():\n",
    "        predict = bert_model(input_ids)[0]\n",
    "    \n",
    "    # Place the predictions in the sentence\n",
    "    for mask_idx in mask_positions:\n",
    "        predicted_words = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "        print(predicted_words)\n",
    "        for idx in range((len(predicted_words))): \n",
    "            new_sentences[idx][mask_idx] = predicted_words[idx]\n",
    "\n",
    "    new_sentences = [sent[1:-1] for sent in new_sentences]  #remove the cls and sep tag\n",
    "    print(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]']]\n",
      "['am', 'feel', 'get', 'was', 'look']\n",
      "['do', 'say', 'think', 'make', '...']\n",
      "[['i', 'am', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do'], ['i', 'feel', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'say'], ['i', 'get', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'think'], ['i', 'was', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'make'], ['i', 'look', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', '...']]\n"
     ]
    }
   ],
   "source": [
    "get_predictions(\"i <mask> confused and don ' t know what to do\",[2, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i <mask> confused and don ' t know what to do\n",
      "[['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]'], ['[CLS]', 'i', '<mask>', 'confused', 'and', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '[SEP]']]\n",
      "['am', 'feel', 'get', 'was', 'look']\n",
      "['do', 'say', 'think', 'make', '...']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-c0644c364a90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_mask_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am confused and don't know what to do\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-a829804cbb56>\u001b[0m in \u001b[0;36mget_mask_predictions\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmask_positions\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_clean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-172-909636fb1a2f>\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(text_sentence, mask_positions, top_clean)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mnew_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mnew_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#remove the cls and sep tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "get_mask_predictions(\"I am confused and don't know what to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "strg = [['asdas','asdasdd']]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "strg = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strg[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence(tokens, tokenizer, style='bert'):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of str, tokenized sentence.\n",
    "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n",
    "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n",
    "\n",
    "    Replace some with <mask>, some with random words.\n",
    "    \"\"\"\n",
    "    output_label = []\n",
    "    mask_positions = [] # For storing the position where words are changed\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = \"<mask>\"\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # Let's store the position where words are changed\n",
    "            mask_positions.append(i)\n",
    "            # append current token to output (we will predict these later)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                output_label.append(tokenizer.vocab[token])\n",
    "            except KeyError:\n",
    "                # For unknown words (should not occur with BPE vocab)\n",
    "                output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "                print(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, mask_positions, output_label \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_input( tokenizer: object, style:str, text: str):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "    * Tokenize the sentence.\n",
    "    * Send it to mask_sentence() -> Get the masked sentence and a\n",
    "      list. This list will have Indices numbers for positions\n",
    "      where masking is done.\n",
    "    * Convert sentences to ids.\n",
    "\n",
    "    Input : \n",
    "        :param -> tokenizer (transformer's object)\n",
    "        :style -> Masking style\n",
    "        :text  -> Sentence\n",
    "    Return: \n",
    "        :param -> Masked sentence, \n",
    "        :param -> Mask labels\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text_ = copy.copy(tokenized_text)\n",
    "    masktokenized_text, mask_positions, mask_labels = mask_sentence(tokenized_text_, tokenizer, style=style)\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(masktokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    # segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "    segments_ids = [0]* len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    input_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "                \n",
    "    # return tokenized_text, masktokenized_text, input_tensor,\\\n",
    "    #         mask_labels, segments_tensors\n",
    "    return masktokenized_text, mask_positions, mask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_predictions(sentence: str):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "    For each model,\n",
    "        * Mask the sentence\n",
    "        * Stitch it to a normal sentence (undo BPE). TODO\n",
    "        * Send it through the model\n",
    "    \"\"\"\n",
    "    masktokenized_text = ''\n",
    "    for style in model_dict.keys():\n",
    "        while '<mask>' not in masktokenized_text:\n",
    "            masktokenized_text, mask_positions, _ = prepare_input(tokenizer=model_dict[style][0], \\\n",
    "                                                            style=f'{style}', text=sentence)\n",
    "            \n",
    "        masked_sentence = ' '.join(masktokenized_text)\n",
    "        print(masked_sentence)\n",
    "        # Adding 1 for every position since <CLS> & <SEP> are added at the ends.\n",
    "        mask_positions =  [(pos+1) for pos in mask_positions]\n",
    "\n",
    "        get_predictions(masked_sentence, mask_positions, top_clean=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am confused and <mask> <mask> t know <mask> to do\n",
      "['i am confused and <mask> <mask> t know <mask> to do', 'i am confused and <mask> <mask> t know <mask> to do', 'i am confused and <mask> <mask> t know <mask> to do', 'i am confused and <mask> <mask> t know <mask> to do', 'i am confused and <mask> <mask> t know <mask> to do']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-c0644c364a90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_mask_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am confused and don't know what to do\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-a829804cbb56>\u001b[0m in \u001b[0;36mget_mask_predictions\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmask_positions\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_clean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-84-4f599cb69ce9>\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(text_sentence, mask_positions, top_clean)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmask_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpredicted_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "get_mask_predictions(\"I am confused and don't know what to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_aug]",
   "language": "python",
   "name": "conda-env-data_aug-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
