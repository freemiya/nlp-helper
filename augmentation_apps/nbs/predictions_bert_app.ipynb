{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "model_dict = {'bert':(bert_tokenizer, bert_model)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return tokens[:top_clean]\n",
    "\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text_sentence, mask_positions, top_clean=5):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "        Get the masked sentence.\n",
    "        Encode it & pass it through model\n",
    "        Now, decode at each position.\n",
    "    \"\"\"\n",
    "    # ========================= BERT =================================\n",
    "    input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
    "    new_sentences = []\n",
    "    for i in range(top_clean): \n",
    "        predicted_sentence = ('[CLS] '+text_sentence+' [SEP]').strip().split()\n",
    "        new_sentences.append(predicted_sentence)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = bert_model(input_ids)[0]\n",
    "    \n",
    "    # Place the predictions in the sentence\n",
    "    for mask_idx in mask_positions:\n",
    "        predicted_words = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "#         print(predicted_words)\n",
    "        for idx in range((len(predicted_words))): \n",
    "            new_sentences[idx][mask_idx] = predicted_words[idx]\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence(tokens, tokenizer, style='bert'):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of str, tokenized sentence.\n",
    "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n",
    "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n",
    "\n",
    "    Replace some with <mask>, some with random words.\n",
    "    \"\"\"\n",
    "    output_label = []\n",
    "    mask_positions = [] # For storing the position where words are changed\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = \"<mask>\"\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # Let's store the position where words are changed\n",
    "            mask_positions.append(i)\n",
    "            # append current token to output (we will predict these later)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                output_label.append(tokenizer.vocab[token])\n",
    "            except KeyError:\n",
    "                # For unknown words (should not occur with BPE vocab)\n",
    "                output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "                print(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, mask_positions, output_label \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_input( tokenizer: object, style:str, text: str):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "    * Tokenize the sentence.\n",
    "    * Send it to mask_sentence() -> Get the masked sentence and a\n",
    "      list. This list will have Indices numbers for positions\n",
    "      where masking is done.\n",
    "    * Convert sentences to ids.\n",
    "\n",
    "    Input : \n",
    "        :param -> tokenizer (transformer's object)\n",
    "        :style -> Masking style\n",
    "        :text  -> Sentence\n",
    "    Return: \n",
    "        :param -> Masked sentence, \n",
    "        :param -> Mask labels\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text_ = copy.copy(tokenized_text)\n",
    "    masktokenized_text, mask_positions, mask_labels = mask_sentence(tokenized_text_, tokenizer, style=style)\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(masktokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    # segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "    segments_ids = [0]* len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    input_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "                \n",
    "    # return tokenized_text, masktokenized_text, input_tensor,\\\n",
    "    #         mask_labels, segments_tensors\n",
    "    return masktokenized_text, mask_positions, mask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_predictions(sentence: str):\n",
    "    \"\"\"\n",
    "    Psuedocode:\n",
    "    For each model,\n",
    "        * Mask the sentence\n",
    "        * Stitch it to a normal sentence (undo BPE). TODO\n",
    "        * Send it through the model\n",
    "    \"\"\"\n",
    "    masktokenized_text = ''\n",
    "    total_preds = []\n",
    "    for style in model_dict.keys():\n",
    "        while '<mask>' not in masktokenized_text:\n",
    "            masktokenized_text, mask_positions, _ = prepare_input(tokenizer=model_dict[style][0], \\\n",
    "                                                            style=f'{style}', text=sentence)\n",
    "            \n",
    "        masked_sentence = ' '.join(masktokenized_text)\n",
    "#         print(masked_sentence)\n",
    "        # Adding 1 for every position since <CLS> & <SEP> are added at encode stage.\n",
    "        mask_positions =  [(pos+1) for pos in mask_positions]\n",
    "\n",
    "        \n",
    "        pred_sents = get_predictions(masked_sentence, mask_positions, top_clean=5)\n",
    "        \n",
    "        #Add html tags to it\n",
    "        for idx, sent in enumerate(pred_sents):\n",
    "            for pos in mask_positions:\n",
    "                pred_sents[idx][pos] =  \"<p style='color:blue; display:inline'><b>\" + pred_sents[idx][pos] + \"</b></p>\"\n",
    "        \n",
    "        pred_sentences = [' '.join(sent[1:-1]) for sent in pred_sents]  #remove the cls and sep tag\n",
    "        total_preds += pred_sentences\n",
    "        \n",
    "\n",
    "    return total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_sents = get_mask_predictions(\"I am confused and don't know what to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textinput = widgets.Text(placeholder='Hello, how are you?', description='Input: ', disabled=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display output of the predictions\n",
    "output = widgets.HTML(\n",
    "    value= \"Augmented sentences will be here\",\n",
    "    placeholder='Some HTML',\n",
    "    description='Output:',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Process!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(*ignore):\n",
    "    new_sents = get_mask_predictions(str(textinput.value))\n",
    "    output.value = '<br>'.join([str(textinput.value)]+new_sents)\n",
    "\n",
    "button.on_click(get_outputs)\n",
    "# textinput.observe(get_outputs, 'value')\n",
    "\n",
    "io = widgets.VBox([textinput, output])\n",
    "box = widgets.HBox([io, button])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74966cc96024d7e89f153c72573eb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value=\"This advertisement looks great! It's too bad that quality doesn't trâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_aug]",
   "language": "python",
   "name": "conda-env-data_aug-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
